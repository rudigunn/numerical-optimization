{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "getcontext().prec = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def steepest_descent(function, f_gradient, x0, epsilon=1e-4, max_iter=100000, c1=1e-4):\n",
    "    \"\"\"\n",
    "    Steepest descent optimization algorithm with Wolfe conditions.\n",
    "    \n",
    "    Args:\n",
    "    - function: Objective function to minimize.\n",
    "    - f_gradient: Function to compute the gradient of the objective function.\n",
    "    - x0: Initial guess.\n",
    "    - epsilon: Convergence criterion based on the norm of the gradient.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "    - c1: Parameter for Armijo condition.\n",
    "    - c2: Parameter for curvature condition.\n",
    "    \n",
    "    Returns:\n",
    "    - x_opt: Optimal solution.\n",
    "    - num_iterations: Number of iterations until convergence.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    num_iterations = 0\n",
    "\n",
    "    gradient = f_gradient(x)\n",
    "    direction = -gradient\n",
    "            \n",
    "    smth_else = direction.T @ direction\n",
    "\n",
    "   \n",
    "    while True:\n",
    "        # Step 1: Check convergence criterion\n",
    "        gradient = f_gradient(x)\n",
    "      \n",
    "        if np.linalg.norm(gradient) < epsilon or num_iterations >= max_iter:\n",
    "            break\n",
    "        \n",
    "        # Step 2: Compute direction (negative gradient)\n",
    "        direction = -gradient\n",
    "        \n",
    "        step_size = smth_else/ direction.T @ direction\n",
    "        # Step 3: Update x with step size satisfying Wolfe conditions\n",
    "        grad_x_dir =  np.dot(gradient, direction)\n",
    "        while True:\n",
    "            # Armijo condition\n",
    "            if function(x + step_size * direction) <= function(x) + c1 * step_size * grad_x_dir:\n",
    "                break      \n",
    "            step_size *= 0.9\n",
    "            # print(\"STEP LENGHT:\", step_size)\n",
    "        smth_else = -step_size * direction.T @ direction\n",
    "\n",
    "        # Step 4: Update x\n",
    "        x = x + step_size * direction\n",
    "        \n",
    "        # Step 5: Increase iteration count\n",
    "        num_iterations += 1\n",
    "\n",
    "  \n",
    "    \n",
    "    print(\"Final gradient norm:\", gradient)   \n",
    "\n",
    "    x = np.array([float(value) for value in x])\n",
    "    \n",
    "    return x, num_iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hilbert matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert_matrix(n):\n",
    "     \"\"\"\n",
    "    Hilbert matrix.\n",
    "\n",
    "    Args:\n",
    "    - n: number of dimensions\n",
    "\n",
    "    Returns:\n",
    "    - Hilbert matrix with n dimensions \n",
    "    \"\"\"\n",
    "     return np.array([[1 / (i + j - 1) for j in range(1, n + 1)] for i in range(1, n + 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient\n",
    "\n",
    "For сonjugate gradient we are using more precise values of numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_solver(A, b, x0, tol=Decimal(1e-6), max_iter=1000):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient Solver for Linear Systems.\n",
    "\n",
    "    This function implements the conjugate gradient method to solve linear systems of the form Ax = b.\n",
    "\n",
    "    Args:\n",
    "    - A: Coefficient matrix of the linear system.\n",
    "    - b: Right-hand side vector of the linear system.\n",
    "    - x0: Initial guess for the solution.\n",
    "    - tol: Tolerance for convergence (default: 1e-6).\n",
    "    - max_iter: Maximum number of iterations (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - x: Approximate solution to the linear system.\n",
    "    - iterations: Number of iterations until convergence.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = x0\n",
    "    r = np.dot(A, x)\n",
    "    r = [Decimal(value) for value in r]\n",
    "    r = np.array(r)\n",
    "    b = np.array([Decimal(value) for value in b])\n",
    "    r = b - r\n",
    "    p = -r\n",
    "    residual_norm = Decimal(np.linalg.norm(r))\n",
    "    iterations = 0\n",
    "    \n",
    "    while residual_norm > tol and iterations < max_iter:\n",
    "    \n",
    "        A_decimal = np.array([[Decimal(value) for value in row] for row in A])\n",
    "        \n",
    "        Ap = A_decimal @ p\n",
    "        \n",
    "        alpha = Decimal(r.T @ r) / (p.T @ Ap)\n",
    "        x = np.array([Decimal(value) for value in x])  \n",
    "        x =  x + alpha * p  \n",
    "        r_new = r + alpha * Ap\n",
    "        beta = Decimal(r_new.T @ r_new) / (r.T @ r)\n",
    "        p = -r_new + beta * p\n",
    "        r = r_new\n",
    "        residual_norm = Decimal(np.linalg.norm(r))\n",
    "        iterations += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"Final gradient norm:\", residual_norm)\n",
    "\n",
    "    x = np.array([float(value) for value in x])\n",
    "        \n",
    "    return x, iterations\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving quadratic function with steepest descent and Conjugate gradient\n",
    "Here we solving quadratic function with different dimensions of Hilbert matrix and comraning with exact solution x* (Qx = b).\n",
    "- **dimensions = [5,8,12,20,30]**\n",
    "- starting points for **SD** is **exact solution + noise**\n",
    "- starting points for **CG** is **zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 5\n",
      "****************************************************************************************************\n",
      "STEEPEST DESCENT\n",
      "\n",
      "Final gradient norm: [1.41803699 0.72833724 0.48738971 0.36450109 0.29017402]\n",
      "Distant from solution SD: 1.8036247812828234\n",
      "Solution SD: [    6.21705584  -119.46123369   629.89900118 -1119.3300038\n",
      "   628.98882612] \n",
      "Number of iterations SD: 100000\n",
      "\n",
      "CONJUGATE GRADIENT\n",
      "Final gradient norm: 4.362585917722405764282883728491949678651014845473771234377266710420888395998294034406012246981006540E-89\n",
      "Distant from solution CG: 2872.368360786777\n",
      "Solution CG: [  -5.  120. -630. 1120. -630.] \n",
      "Number of iterations CG: 5\n",
      "____________________________________________________________________________________________________\n",
      "Dimension: 8\n",
      "****************************************************************************************************\n",
      "STEEPEST DESCENT\n",
      "\n",
      "Final gradient norm: [-0.56592479 -0.39465496 -0.32206851 -0.27464184 -0.23978083 -0.21278235\n",
      " -0.1911925  -0.17352275]\n",
      "Distant from solution SD: 3.3973942205769925\n",
      "Solution SD: [-8.50536982e+00  5.05909393e+02 -7.56196386e+03  4.61992063e+04\n",
      " -1.38601377e+05  2.16216062e+05 -1.68168242e+05  5.14810995e+04] \n",
      "Number of iterations SD: 100000\n",
      "\n",
      "CONJUGATE GRADIENT\n",
      "Final gradient norm: 1.309420974602396433956287669219183175133484114428174506009501768866139937488048484667408961424743930E-66\n",
      "Distant from solution CG: 629545.3411989687\n",
      "Solution CG: [ 7.99999995e+00 -5.03999995e+02  7.55999992e+03 -4.61999995e+04\n",
      "  1.38599998e+05 -2.16215997e+05  1.68167998e+05 -5.14799994e+04] \n",
      "Number of iterations CG: 8\n",
      "____________________________________________________________________________________________________\n",
      "Dimension: 12\n",
      "****************************************************************************************************\n",
      "STEEPEST DESCENT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25056\\1872034599.py:27: LinAlgWarning: Ill-conditioned matrix (rcond=2.34967e-17): result may not be accurate.\n",
      "  exact_solution = solve(Q, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final gradient norm: [0.79721741 0.4827401  0.3376216  0.25266504 0.19711705 0.15828361\n",
      " 0.12986094 0.10834201 0.09161704 0.0783415  0.0676196  0.05883284]\n",
      "Distant from solution SD: 3.524533336591125\n",
      "Solution SD: [-1.26593384e+01  1.82785893e+03 -6.35492523e+04  9.48423843e+05\n",
      " -7.55539363e+06  3.58352877e+07 -1.07176655e+08  2.07239527e+08\n",
      " -2.58446727e+08  2.00601047e+08 -8.81030655e+07  1.67194335e+07] \n",
      "Number of iterations SD: 100000\n",
      "\n",
      "CONJUGATE GRADIENT\n",
      "Final gradient norm: 6.466145993833794101868851712656275895582491757266270669370003845468053632973568506372315384982178401E-7\n",
      "Distant from solution CG: 413389745.8508265\n",
      "Solution CG: [ 9.60881796e+00 -8.15395474e+02  1.64965819e+04 -1.35510565e+05\n",
      "  5.36482077e+05 -1.02540089e+06  6.42579602e+05  6.57590809e+05\n",
      " -8.04244710e+05 -6.63072100e+05  1.24127990e+06 -4.65506744e+05] \n",
      "Number of iterations CG: 10\n",
      "____________________________________________________________________________________________________\n",
      "Dimension: 20\n",
      "****************************************************************************************************\n",
      "STEEPEST DESCENT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25056\\1872034599.py:27: LinAlgWarning: Ill-conditioned matrix (rcond=2.93284e-20): result may not be accurate.\n",
      "  exact_solution = solve(Q, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final gradient norm: [-6.39057887 -4.88600671 -4.08309138 -3.54974246 -3.15942597 -2.85712552\n",
      " -2.61397815 -2.4130134  -2.2434473  -2.09802777 -1.97166055 -1.86064246\n",
      " -1.76220521 -1.67423069 -1.59506583 -1.5233995  -1.45817551 -1.39853275\n",
      " -1.34376103 -1.29326907]\n",
      "Distant from solution SD: 8.81788187624377\n",
      "Solution SD: [-2.50133356e+01  4.30168295e+03 -1.89868192e+05  3.62631738e+06\n",
      " -3.72981120e+07  2.29900619e+08 -8.93067433e+08  2.19698466e+09\n",
      " -3.19974825e+09  1.90348954e+09  1.66222839e+09 -3.44431180e+09\n",
      "  6.70327262e+08  2.80681504e+09 -2.55090193e+09  7.25686986e+08\n",
      " -7.41746060e+08  1.37490617e+09 -9.17188996e+08  2.10483375e+08] \n",
      "Number of iterations SD: 100000\n",
      "\n",
      "CONJUGATE GRADIENT\n",
      "Final gradient norm: 6.247623538244119522042585815978123929295687631917687020417270444132947613600915563476081535989414510E-7\n",
      "Distant from solution CG: 7270082119.249533\n",
      "Solution CG: [ 1.09739846e+01 -1.05086504e+03  2.39554764e+04 -2.20422255e+05\n",
      "  9.65342023e+05 -1.99010742e+06  1.25271616e+06  1.34346727e+06\n",
      " -8.83244100e+05 -1.68796283e+06 -3.88198421e+05  1.30553075e+06\n",
      "  1.71053598e+06  5.28234419e+05 -1.20868745e+06 -2.00287547e+06\n",
      " -9.44575107e+05  1.43404666e+06  2.65092458e+06 -1.88783965e+06] \n",
      "Number of iterations CG: 12\n",
      "____________________________________________________________________________________________________\n",
      "Dimension: 30\n",
      "****************************************************************************************************\n",
      "STEEPEST DESCENT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25056\\1872034599.py:27: LinAlgWarning: Ill-conditioned matrix (rcond=1.55035e-19): result may not be accurate.\n",
      "  exact_solution = solve(Q, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final gradient norm: [-4.2194893  -2.76711074 -2.13521703 -1.7646059  -1.51598538 -1.33554241\n",
      " -1.19756242 -1.08804374 -0.99864675 -0.92406409 -0.86074328 -0.80620756\n",
      " -0.75867332 -0.71681941 -0.67964526 -0.64637739 -0.61640789 -0.58925147\n",
      " -0.56451589 -0.54188    -0.52107844 -0.50188979 -0.48412754 -0.46763331\n",
      " -0.45227226 -0.43792798 -0.42449995 -0.41190074 -0.40005396 -0.38889244]\n",
      "Distant from solution SD: 6.861125767249022\n",
      "Solution SD: [-1.78796560e+01  2.50444748e+03 -9.30056137e+04  1.43443556e+06\n",
      " -1.11133348e+07  4.54428958e+07 -8.72613457e+07  7.20189745e+06\n",
      "  2.63016791e+08 -4.12804564e+08  4.65909807e+08 -1.32778299e+09\n",
      "  2.22340808e+09 -9.67189566e+08 -1.33485669e+08 -1.54773867e+09\n",
      "  1.73502252e+09 -2.33952925e+08  1.89805272e+09 -2.36541695e+09\n",
      " -1.06047520e+09  1.40449738e+09  1.06823900e+09 -7.88639369e+08\n",
      " -7.70915773e+08  4.85022973e+08  6.42532594e+08 -7.28502510e+08\n",
      "  1.63214926e+08  3.23737176e+07] \n",
      "Number of iterations SD: 100000\n",
      "\n",
      "CONJUGATE GRADIENT\n",
      "Final gradient norm: 2.864302935740296926499958695715662057339428309088113494804539498494076172285902721971049742533625132E-7\n",
      "Distant from solution CG: 5426701644.32364\n",
      "Solution CG: [ 1.25472353e+01 -1.38677741e+03  3.66914611e+04 -3.96073695e+05\n",
      "  2.07970076e+06 -5.43186018e+06  5.71609484e+06  1.60053200e+06\n",
      " -4.84425922e+06 -3.47426500e+06  1.96778022e+06  4.97807976e+06\n",
      "  3.39517904e+06 -7.57739137e+05 -4.23428992e+06 -4.94852206e+06\n",
      " -2.77374110e+06  9.01821704e+05  4.14643632e+06  5.41791210e+06\n",
      "  4.14136733e+06  8.49863110e+05 -3.05417717e+06 -5.79981470e+06\n",
      " -5.88118838e+06 -2.71284524e+06  2.77596834e+06  7.65864247e+06\n",
      "  6.66518005e+06 -8.02142157e+06] \n",
      "Number of iterations CG: 14\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dimensions = [5,8,12,20,30]\n",
    "# dimensions = [5,8,12]\n",
    "\n",
    "for n in dimensions:\n",
    "    # Generate Hilbert matrix Q and vector b\n",
    "    Q = hilbert_matrix(n)\n",
    "    b = np.ones(n)\n",
    "\n",
    "    x0 = np.zeros(n)\n",
    "\n",
    "\n",
    "    #Quadratic function and it's gradient\n",
    "\n",
    "    def quadratic_function(x):\n",
    "        return 0.5 * np.dot(x, np.dot(Q, x)) - np.dot(b, x)\n",
    "\n",
    "    \n",
    "    def gradient_quadratic_function(x):\n",
    "        return np.dot(Q, x) - b\n",
    "    \n",
    "  \n",
    "    print(f\"Dimension: {n}\") \n",
    "    print('*'*100)\n",
    "    print(\"STEEPEST DESCENT\\n\")\n",
    "\n",
    "    # Solve linear system Qx = b to find exact solution x*\n",
    "    exact_solution = solve(Q, b) \n",
    "    noise = np.random.normal(0, 1, size=n) \n",
    "    x0_SD = exact_solution + noise  \n",
    "    \n",
    "    # print(\"Starting points\", x0)\n",
    "    # print(\"\\n\")\n",
    "    # print(\"Exact solution :\",exact_solution)\n",
    "    \n",
    "    # Optimize using steepest descent\n",
    "    solution_SD, num_iterations_SD = steepest_descent(quadratic_function, \n",
    "                                                gradient_quadratic_function, \n",
    "                                                x0_SD)\n",
    "    dist_to_solution_SD = np.linalg.norm(exact_solution - solution_SD) \n",
    "    print(\"Distant from solution SD:\", dist_to_solution_SD)\n",
    "    \n",
    "    print(f\"Solution SD: {solution_SD} \\nNumber of iterations SD: {num_iterations_SD}\")\n",
    "\n",
    "    print(\"\\nCONJUGATE GRADIENT\")\n",
    "    \n",
    "    x0_CG = np.zeros(n)\n",
    "    solution_CG, num_iterations_CG = cg_solver(Q, b, x0_CG)\n",
    "    dist_to_solution_CG = np.linalg.norm(exact_solution - solution_CG) \n",
    "    print(\"Distant from solution CG:\", dist_to_solution_CG)\n",
    "    print(f\"Solution CG: {solution_CG} \\nNumber of iterations CG: {num_iterations_CG}\")\n",
    "\n",
    "\n",
    "    print('_'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compared the steepest descent and conjugate gradient methods for solving quadratic functions, we noticed some big differences. With the steepest descent method, it used up all its chances to solve the quadratic function, no matter how many dimensions the Hilbert matrix had. However, when we reduced the dimensions to 5, both methods gave similar results. But when we went beyond 5 dimensions, we saw significant differences in the solutions provided by these two methods. This shows that the steepest descent and conjugate gradient methods behave quite differently when solving quadratic functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
