{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import hessian, jacobian\n",
    "from scipy.linalg import hilbert\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class GradStop():\n",
    "\n",
    "    def __init__(self, f = None, suf_grad = 1e-6) -> None:\n",
    "        self.f = f\n",
    "        self.suf_grad = suf_grad\n",
    "\n",
    "    def __call__(self, x, verbose = False) -> Any:\n",
    "        grad_norm = np.linalg.norm(jacobian(self.f, x).detach().numpy())\n",
    "        solved = grad_norm < self.suf_grad\n",
    "        #if solved and verbose:\n",
    "        if verbose: print(f\"Gradient norm {grad_norm}.\")\n",
    "        return solved \n",
    "\n",
    "\n",
    "\n",
    "class Solver():\n",
    "\n",
    "    def __init__(self, f, stop_crit, max_iter = 10_000, ro = 0.5, alpha_ini = 0.99, c = 0.45, alpha_f = None, iternad_processor = None) -> None:\n",
    "        self.f = f\n",
    "        self.stop_crit = stop_crit\n",
    "        if self.stop_crit.f == None: self.stop_crit.f = f\n",
    "        self.ro = ro\n",
    "        self.alpha_ini = alpha_ini\n",
    "        self.c = c\n",
    "        self.iterands = 0\n",
    "        self.max_iter = max_iter\n",
    "        if alpha_f: self.get_alpha = alpha_f\n",
    "        self.has_iterand_processor = iternad_processor is not None\n",
    "        if iternad_processor is not None:\n",
    "            self.iterand_processor = iternad_processor\n",
    "\n",
    "    def solve(self, x):\n",
    "        x = self.tensorize(x)\n",
    "        self.iterands = 0\n",
    "        while not self.stop_crit(x) and self.iterands < self.max_iter:\n",
    "            #print(f\".x_{len(self.iterands)} = {x}\")\n",
    "            self.iterands += 1\n",
    "            if self.has_iterand_processor:\n",
    "                self.iterand_processor(x)\n",
    "            p = torch.reshape(self.get_p(x), x.shape)\n",
    "            x = x + self.get_alpha(p, x) * p\n",
    "        if self.stop_crit(x, verbose = True):\n",
    "            print(f\"Converged to the solution {x} after {self.iterands} steps\")\n",
    "        else:\n",
    "            print(f\"Failed to converge and ended in {x}\")\n",
    "        return x\n",
    "\n",
    "    def get_p(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_alpha(self, p, x):\n",
    "        # Do the line search\n",
    "        alpha = self.alpha_ini\n",
    "        while self.f(x + alpha * p) > self.f(x) + self.c * alpha * p.T @ jacobian(self.f,x):\n",
    "            alpha *= self.ro\n",
    "        return alpha\n",
    "\n",
    "    def tensorize(self, x):\n",
    "        if type(x) in [int, float]: return torch.DoubleTensor([x])\n",
    "        else: return torch.DoubleTensor(x)\n",
    "\n",
    "class HessianModifiedNewton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = hessian(self.f, x)\n",
    "        modified_hes = self.make_positive_definite(hes)\n",
    "        down_grad = -jacobian(self.f, x)\n",
    "        p = np.linalg.solve(modified_hes, down_grad)\n",
    "        return torch.DoubleTensor(p)\n",
    "    \n",
    "    def make_positive_definite(self, H, beta = 1e-3, max_iter = 1e4):\n",
    "        # Choose tau\n",
    "        min_diag = np.min(np.diag(H))\n",
    "        tau = 0 if min_diag > 0 else beta - min_diag\n",
    "        I = np.eye(*H.shape)\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            try:\n",
    "                L = np.linalg.cholesky(H + tau * I)\n",
    "                return L @ L.T\n",
    "            except np.linalg.LinAlgError:\n",
    "                tau = max(2 * tau, beta)\n",
    "            i += 1\n",
    "\n",
    "class Newton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = hessian(self.f, x)\n",
    "        down_grad = -jacobian(self.f, x)\n",
    "        p = np.linalg.solve(hes, down_grad)\n",
    "        return torch.DoubleTensor(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 32 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 2.671230645372248.\n",
      "Failed to converge and ended in tensor([0.1948, 0.0455], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = Newton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 25.90677839441986.\n",
      "Failed to converge and ended in tensor([-0.1576,  0.7289], dtype=torch.float64)\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 34.954029214106214.\n",
      "Failed to converge and ended in tensor([1.4726, 0.0607], dtype=torch.float64)\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 0.006535527940709905.\n",
      "Failed to converge and ended in tensor([0.4366, 0.1094], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = Newton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method with Hessian Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 32 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 1.1491707515913118e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 39 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 8.246042296740478e-08.\n",
      "Converged to the solution tensor([2.6682e-10, 1.0000e+00], dtype=torch.float64) after 7 steps\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 8.949292743208082e-07.\n",
      "Converged to the solution tensor([4.0000e+00, 1.8204e-10], dtype=torch.float64) after 13 steps\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 1.354512588685944e-08.\n",
      "Converged to the solution tensor([ 4.0000e+00, -2.9169e-12], dtype=torch.float64) after 21 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
