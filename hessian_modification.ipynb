{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import hessian, jacobian\n",
    "from scipy.linalg import hilbert\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 1.0\n",
    "while 1.0 + u != 1.0:\n",
    "    u /= 2\n",
    "epsilon = np.sqrt(u)\n",
    "    \n",
    "# Print the unit roundoff value\n",
    "print(\"Manually calculated unit roundoff value (u):\", u)\n",
    "print(\"Epsilon from manually calculated u:\", epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_difference(func: callable, x: np.ndarray, epsilon: float = epsilon) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    func: Function you want to approximate the derivative of\n",
    "    x: Point where at which you want the approximation of the derivative\n",
    "    epsilon: very small number typically sqrt(u), where u is unit-roundoff\n",
    "    Returns:\n",
    "    Approximation of derivative at point x\n",
    "    \"\"\"\n",
    "    dim = x.shape[0] if x.shape != () else 1\n",
    "    grad = np.zeros(dim)\n",
    "    \n",
    "    for i in range(dim):\n",
    "        e = np.zeros(dim)\n",
    "        e[i] = 1\n",
    "        grad_i = (func(x + epsilon * e) - func(x)) / epsilon\n",
    "        grad[i] = grad_i\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def hessian_approximation_wiki(func: callable, x: np.ndarray, epsilon: float = 1e-4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    func: Function you want to approximate the derivative of\n",
    "    x: Point where at which you want the approximation of the derivative\n",
    "    epsilon: CAREFUL WITH CHOISE OF EPSILON!!!\n",
    "             Using the above define u results in very bad results, best results after testing with 1e-4\n",
    "    Returns:\n",
    "    Approximation of hessian at point x\n",
    "    \"\"\"\n",
    "    dim = x.shape[0] if x.shape != () else 1\n",
    "    hessian = np.zeros((dim, dim))\n",
    "    \n",
    "    # diag elements\n",
    "    for i in range(dim):\n",
    "        e_i = np.zeros(dim)\n",
    "        e_i[i] = 1\n",
    "        term1 = func(x + epsilon * e_i)\n",
    "        term2 = 2 * func(x)\n",
    "        term3 = func(x - epsilon * e_i)\n",
    "        hessian[i, i] = (term1 - term2 + term3) / (epsilon**2)\n",
    "    \n",
    "    # off-diag elements\n",
    "    for i in range(dim):\n",
    "        e_i = np.zeros(dim)\n",
    "        e_i[i] = 1\n",
    "        for j in range(i+1, dim):\n",
    "            e_j = np.zeros(dim)\n",
    "            e_j[j] = 1\n",
    "            term1 = func(x + epsilon * (e_i + e_j))\n",
    "            term2 = func(x + epsilon * (e_i - e_j))\n",
    "            term3 = func(x + epsilon * (-e_i + e_j))\n",
    "            term4 = func(x + epsilon * (-e_i - e_j))\n",
    "            hessian[i, j] = hessian[j, i] = (term1 - term2 - term3 + term4) / (4 * epsilon**2)\n",
    "        \n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class GradStop():\n",
    "\n",
    "    def __init__(self, f = None, suf_grad = 1e-6) -> None:\n",
    "        self.f = f\n",
    "        self.suf_grad = suf_grad\n",
    "        self.grad_f = jacobian\n",
    "\n",
    "    def __call__(self, x, verbose = False) -> Any:\n",
    "        grad = self.grad_f(self.f, x)\n",
    "        if isinstance(grad, torch.DoubleTensor): grad = grad.detach().numpy()\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        solved = grad_norm < self.suf_grad\n",
    "        #if solved and verbose:\n",
    "        if verbose: print(f\"Gradient norm {grad_norm}.\")\n",
    "        return solved \n",
    "\n",
    "\n",
    "\n",
    "class Solver():\n",
    "\n",
    "    def __init__(self, f, stop_crit, max_iter = 10_000, ro = 0.5, alpha_ini = 0.99, c = 0.45, alpha_f = None, iternad_processor = None, grad_f = jacobian, hes_f = hessian) -> None:\n",
    "        self.f = f\n",
    "        self.stop_crit = stop_crit\n",
    "        if self.stop_crit.f == None: self.stop_crit.f = f\n",
    "        self.ro = ro\n",
    "        self.alpha_ini = alpha_ini\n",
    "        self.c = c\n",
    "        self.iterands = 0\n",
    "        self.max_iter = max_iter\n",
    "        self.grad_f = grad_f\n",
    "        self.stop_crit.grad_f = grad_f\n",
    "        self.hes_f = hes_f\n",
    "        if alpha_f: self.get_alpha = alpha_f\n",
    "        self.has_iterand_processor = iternad_processor is not None\n",
    "        if iternad_processor is not None:\n",
    "            self.iterand_processor = iternad_processor\n",
    "\n",
    "    def solve(self, x):\n",
    "        x = self.tensorize(x)\n",
    "        self.iterands = 0\n",
    "        while not self.stop_crit(x) and self.iterands < self.max_iter:\n",
    "            #print(f\".x_{len(self.iterands)} = {x}\")\n",
    "            self.iterands += 1\n",
    "            if self.has_iterand_processor:\n",
    "                self.iterand_processor(x)\n",
    "            p = torch.reshape(self.get_p(x), x.shape)\n",
    "            x = x + self.get_alpha(p, x) * p\n",
    "        if self.stop_crit(x, verbose = True):\n",
    "            print(f\"Converged to the solution {x} after {self.iterands} steps\")\n",
    "        else:\n",
    "            print(f\"Failed to converge and ended in {x}\")\n",
    "        return x\n",
    "\n",
    "    def get_p(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_alpha(self, p, x):\n",
    "        # Do the line search\n",
    "        alpha = self.alpha_ini\n",
    "        while self.f(x + alpha * p) > self.f(x) + self.c * alpha * p.T @ self.grad_f(self.f,x):\n",
    "            alpha *= self.ro\n",
    "        return alpha\n",
    "\n",
    "    def tensorize(self, x):\n",
    "        if type(x) in [int, float]: return torch.DoubleTensor([x])\n",
    "        else: return torch.DoubleTensor(x)\n",
    "\n",
    "class HessianModifiedNewton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = self.hes_f(self.f, x)\n",
    "        modified_hes = self.make_positive_definite(hes)\n",
    "        down_grad = -self.grad_f(self.f, x)\n",
    "        #print(\"Approximated:\", down_grad)\n",
    "        #print(\"Exact:\", -jacobian(self.f, x))\n",
    "        p = np.linalg.solve(modified_hes, down_grad)\n",
    "        return torch.DoubleTensor(p)\n",
    "    \n",
    "    def make_positive_definite(self, H, beta = 1e-3, max_iter = 1e4):\n",
    "        # Choose tau\n",
    "        min_diag = np.min(np.diag(H))\n",
    "        tau = 0 if min_diag > 0 else beta - min_diag\n",
    "        I = np.eye(*H.shape)\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            try:\n",
    "                L = np.linalg.cholesky(H + tau * I)\n",
    "                return L @ L.T\n",
    "            except np.linalg.LinAlgError:\n",
    "                tau = max(2 * tau, beta)\n",
    "            i += 1\n",
    "\n",
    "class Newton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = self.hes_f(self.f, x)\n",
    "        down_grad = -self.grad_f(self.f, x)\n",
    "        p = np.linalg.solve(hes, down_grad)\n",
    "        return torch.DoubleTensor(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 23 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 2.671230645372209.\n",
      "Failed to converge and ended in tensor([0.1948, 0.0455], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = Newton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m solver \u001b[38;5;241m=\u001b[39m Newton(problem_1, GradStop(problem_1), grad_f\u001b[38;5;241m=\u001b[39mforward_difference, hes_f \u001b[38;5;241m=\u001b[39m hessian_approximation_wiki)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting at (1.2, 1.2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting at (-1.2, 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39msolve([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn [17], line 41\u001b[0m, in \u001b[0;36mSolver.solve\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensorize(x)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterands \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_crit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterands \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m#print(f\".x_{len(self.iterands)} = {x}\")\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterands \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_iterand_processor:\n",
      "Cell \u001b[1;32mIn [17], line 11\u001b[0m, in \u001b[0;36mGradStop.__call__\u001b[1;34m(self, x, verbose)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 11\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     12\u001b[0m     solved \u001b[38;5;241m=\u001b[39m grad_norm \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuf_grad\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#if solved and verbose:\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = Newton(problem_1, GradStop(problem_1), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 25.90677839441986.\n",
      "Failed to converge and ended in tensor([-0.1576,  0.7289], dtype=torch.float64)\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 34.954029214106214.\n",
      "Failed to converge and ended in tensor([1.4726, 0.0607], dtype=torch.float64)\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 0.006535527940709905.\n",
      "Failed to converge and ended in tensor([0.4366, 0.1094], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = Newton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = Newton(problem_2, GradStop(problem_2), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method with Hessian Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Approximated: tensor([-115.6000,   48.0000], dtype=torch.float64)\n",
      "Exact: tensor([-115.6000,   48.0000], dtype=torch.float64)\n",
      "Approximated: tensor([-1.5479,  0.4833], dtype=torch.float64)\n",
      "Exact: tensor([-1.5479,  0.4833], dtype=torch.float64)\n",
      "Approximated: tensor([-2.7470,  1.0994], dtype=torch.float64)\n",
      "Exact: tensor([-2.7470,  1.0994], dtype=torch.float64)\n",
      "Approximated: tensor([-1.7824,  0.7692], dtype=torch.float64)\n",
      "Exact: tensor([-1.7824,  0.7692], dtype=torch.float64)\n",
      "Approximated: tensor([-0.6910,  0.3058], dtype=torch.float64)\n",
      "Exact: tensor([-0.6910,  0.3058], dtype=torch.float64)\n",
      "Approximated: tensor([-0.2347,  0.1092], dtype=torch.float64)\n",
      "Exact: tensor([-0.2347,  0.1092], dtype=torch.float64)\n",
      "Approximated: tensor([-0.0210,  0.0097], dtype=torch.float64)\n",
      "Exact: tensor([-0.0210,  0.0097], dtype=torch.float64)\n",
      "Approximated: tensor([-0.0005,  0.0002], dtype=torch.float64)\n",
      "Exact: tensor([-0.0005,  0.0002], dtype=torch.float64)\n",
      "Approximated: tensor([-4.7451e-06,  2.2152e-06], dtype=torch.float64)\n",
      "Exact: tensor([-4.7451e-06,  2.2152e-06], dtype=torch.float64)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Approximated: tensor([215.6000,  88.0000], dtype=torch.float64)\n",
      "Exact: tensor([215.6000,  88.0000], dtype=torch.float64)\n",
      "Approximated: tensor([6.7016, 0.9998], dtype=torch.float64)\n",
      "Exact: tensor([6.7016, 0.9998], dtype=torch.float64)\n",
      "Approximated: tensor([13.4518,  4.5009], dtype=torch.float64)\n",
      "Exact: tensor([13.4518,  4.5009], dtype=torch.float64)\n",
      "Approximated: tensor([19.1768,  9.0185], dtype=torch.float64)\n",
      "Exact: tensor([19.1768,  9.0185], dtype=torch.float64)\n",
      "Approximated: tensor([12.5460,  6.8267], dtype=torch.float64)\n",
      "Exact: tensor([12.5460,  6.8267], dtype=torch.float64)\n",
      "Approximated: tensor([11.2703,  9.0325], dtype=torch.float64)\n",
      "Exact: tensor([11.2703,  9.0325], dtype=torch.float64)\n",
      "Approximated: tensor([5.3383, 4.2531], dtype=torch.float64)\n",
      "Exact: tensor([5.3383, 4.2531], dtype=torch.float64)\n",
      "Approximated: tensor([4.4125, 5.2315], dtype=torch.float64)\n",
      "Exact: tensor([4.4125, 5.2315], dtype=torch.float64)\n",
      "Approximated: tensor([2.0651, 7.2437], dtype=torch.float64)\n",
      "Exact: tensor([2.0651, 7.2437], dtype=torch.float64)\n",
      "Approximated: tensor([1.0718, 2.9797], dtype=torch.float64)\n",
      "Exact: tensor([1.0718, 2.9797], dtype=torch.float64)\n",
      "Approximated: tensor([-0.2284,  3.9193], dtype=torch.float64)\n",
      "Exact: tensor([-0.2284,  3.9193], dtype=torch.float64)\n",
      "Approximated: tensor([-2.4985,  4.8856], dtype=torch.float64)\n",
      "Exact: tensor([-2.4985,  4.8856], dtype=torch.float64)\n",
      "Approximated: tensor([-1.1196,  2.2089], dtype=torch.float64)\n",
      "Exact: tensor([-1.1196,  2.2089], dtype=torch.float64)\n",
      "Approximated: tensor([-5.7996,  5.0496], dtype=torch.float64)\n",
      "Exact: tensor([-5.7996,  5.0496], dtype=torch.float64)\n",
      "Approximated: tensor([-0.4273,  0.7269], dtype=torch.float64)\n",
      "Exact: tensor([-0.4273,  0.7269], dtype=torch.float64)\n",
      "Approximated: tensor([-2.4421,  1.8187], dtype=torch.float64)\n",
      "Exact: tensor([-2.4421,  1.8187], dtype=torch.float64)\n",
      "Approximated: tensor([-1.6693,  1.1272], dtype=torch.float64)\n",
      "Exact: tensor([-1.6693,  1.1272], dtype=torch.float64)\n",
      "Approximated: tensor([-1.3924,  0.8309], dtype=torch.float64)\n",
      "Exact: tensor([-1.3924,  0.8309], dtype=torch.float64)\n",
      "Approximated: tensor([-0.5596,  0.3245], dtype=torch.float64)\n",
      "Exact: tensor([-0.5596,  0.3245], dtype=torch.float64)\n",
      "Approximated: tensor([-0.2421,  0.1307], dtype=torch.float64)\n",
      "Exact: tensor([-0.2421,  0.1307], dtype=torch.float64)\n",
      "Approximated: tensor([-0.0228,  0.0125], dtype=torch.float64)\n",
      "Exact: tensor([-0.0228,  0.0125], dtype=torch.float64)\n",
      "Approximated: tensor([-0.0006,  0.0003], dtype=torch.float64)\n",
      "Exact: tensor([-0.0006,  0.0003], dtype=torch.float64)\n",
      "Approximated: tensor([-6.5308e-06,  3.5088e-06], dtype=torch.float64)\n",
      "Exact: tensor([-6.5308e-06,  3.5088e-06], dtype=torch.float64)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 23 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Approximated: tensor([  62.4000, -152.0000], dtype=torch.float64)\n",
      "Exact: tensor([  62.4000, -152.0000], dtype=torch.float64)\n",
      "Approximated: tensor([ 72.4108, -95.1009], dtype=torch.float64)\n",
      "Exact: tensor([ 72.4108, -95.1009], dtype=torch.float64)\n",
      "Approximated: tensor([-73.4668,  36.7741], dtype=torch.float64)\n",
      "Exact: tensor([-73.4668,  36.7741], dtype=torch.float64)\n",
      "Approximated: tensor([-0.7326,  0.3677], dtype=torch.float64)\n",
      "Exact: tensor([-0.7326,  0.3677], dtype=torch.float64)\n",
      "Approximated: tensor([-0.0070,  0.0038], dtype=torch.float64)\n",
      "Exact: tensor([-0.0070,  0.0038], dtype=torch.float64)\n",
      "Approximated: tensor([-1.0055e-04,  5.4256e-05], dtype=torch.float64)\n",
      "Exact: tensor([-1.0055e-04,  5.4256e-05], dtype=torch.float64)\n",
      "Approximated: tensor([-1.0113e-06,  5.4566e-07], dtype=torch.float64)\n",
      "Exact: tensor([-1.0113e-06,  5.4566e-07], dtype=torch.float64)\n",
      "Gradient norm 1.1491707515913118e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 7 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m solver \u001b[38;5;241m=\u001b[39m HessianModifiedNewton(problem_1, GradStop(problem_1), grad_f\u001b[38;5;241m=\u001b[39mforward_difference, hes_f \u001b[38;5;241m=\u001b[39m hessian_approximation_wiki)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting at (1.2, 1.2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting at (-1.2, 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39msolve([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn [27], line 49\u001b[0m, in \u001b[0;36mSolver.solve\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterand_processor(x)\n\u001b[0;32m     48\u001b[0m     p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_p(x), x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m p\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_crit(x, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverged to the solution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterands\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [27], line 62\u001b[0m, in \u001b[0;36mSolver.get_alpha\u001b[1;34m(self, p, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_alpha\u001b[39m(\u001b[38;5;28mself\u001b[39m, p, x):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Do the line search\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_ini\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(x \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m p) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     63\u001b[0m         alpha \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mro\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "Cell \u001b[1;32mIn [24], line 16\u001b[0m, in \u001b[0;36mforward_difference\u001b[1;34m(func, x, epsilon)\u001b[0m\n\u001b[0;32m     14\u001b[0m     e \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(dim)\n\u001b[0;32m     15\u001b[0m     e[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     grad_i \u001b[38;5;241m=\u001b[39m (\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m func(x)) \u001b[38;5;241m/\u001b[39m epsilon\n\u001b[0;32m     17\u001b[0m     grad[i] \u001b[38;5;241m=\u001b[39m grad_i\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "Cell \u001b[1;32mIn [28], line 2\u001b[0m, in \u001b[0;36mproblem_1\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproblem_1\u001b[39m(x):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\42073\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:38\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_1, GradStop(problem_1), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 8.246042296740478e-08.\n",
      "Converged to the solution tensor([2.6682e-10, 1.0000e+00], dtype=torch.float64) after 7 steps\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 8.949292743208082e-07.\n",
      "Converged to the solution tensor([4.0000e+00, 1.8204e-10], dtype=torch.float64) after 13 steps\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 1.354512588685944e-08.\n",
      "Converged to the solution tensor([ 4.0000e+00, -2.9169e-12], dtype=torch.float64) after 21 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
