{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import hessian, jacobian\n",
    "from scipy.linalg import hilbert\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually calculated unit roundoff value (u): 1.1102230246251565e-16\n",
      "Epsilon from manually calculated u: 1.0536712127723509e-08\n"
     ]
    }
   ],
   "source": [
    "u = 1.0\n",
    "while 1.0 + u != 1.0:\n",
    "    u /= 2\n",
    "epsilon = np.sqrt(u)\n",
    "    \n",
    "# Print the unit roundoff value\n",
    "print(\"Manually calculated unit roundoff value (u):\", u)\n",
    "print(\"Epsilon from manually calculated u:\", epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_difference(func: callable, x: np.ndarray, epsilon: float = epsilon) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    func: Function you want to approximate the derivative of\n",
    "    x: Point where at which you want the approximation of the derivative\n",
    "    epsilon: very small number typically sqrt(u), where u is unit-roundoff\n",
    "    Returns:\n",
    "    Approximation of derivative at point x\n",
    "    \"\"\"\n",
    "    dim = x.shape[0] if x.shape != () else 1\n",
    "    grad = np.zeros(dim)\n",
    "    \n",
    "    for i in range(dim):\n",
    "        e = np.zeros(dim)\n",
    "        e[i] = 1\n",
    "        grad_i = (func(x + epsilon * e) - func(x)) / epsilon\n",
    "        grad[i] = grad_i\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def hessian_approximation_wiki(func: callable, x: np.ndarray, epsilon: float = 1e-4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    func: Function you want to approximate the derivative of\n",
    "    x: Point where at which you want the approximation of the derivative\n",
    "    epsilon: CAREFUL WITH CHOISE OF EPSILON!!!\n",
    "             Using the above define u results in very bad results, best results after testing with 1e-4\n",
    "    Returns:\n",
    "    Approximation of hessian at point x\n",
    "    \"\"\"\n",
    "    dim = x.shape[0] if x.shape != () else 1\n",
    "    hessian = np.zeros((dim, dim))\n",
    "    \n",
    "    # diag elements\n",
    "    for i in range(dim):\n",
    "        e_i = np.zeros(dim)\n",
    "        e_i[i] = 1\n",
    "        term1 = func(x + epsilon * e_i)\n",
    "        term2 = 2 * func(x)\n",
    "        term3 = func(x - epsilon * e_i)\n",
    "        hessian[i, i] = (term1 - term2 + term3) / (epsilon**2)\n",
    "    \n",
    "    # off-diag elements\n",
    "    for i in range(dim):\n",
    "        e_i = np.zeros(dim)\n",
    "        e_i[i] = 1\n",
    "        for j in range(i+1, dim):\n",
    "            e_j = np.zeros(dim)\n",
    "            e_j[j] = 1\n",
    "            term1 = func(x + epsilon * (e_i + e_j))\n",
    "            term2 = func(x + epsilon * (e_i - e_j))\n",
    "            term3 = func(x + epsilon * (-e_i + e_j))\n",
    "            term4 = func(x + epsilon * (-e_i - e_j))\n",
    "            hessian[i, j] = hessian[j, i] = (term1 - term2 - term3 + term4) / (4 * epsilon**2)\n",
    "        \n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class GradStop():\n",
    "\n",
    "    def __init__(self, f = None, suf_grad = 1e-6) -> None:\n",
    "        self.f = f\n",
    "        self.suf_grad = suf_grad\n",
    "        self.grad_f = jacobian\n",
    "\n",
    "    def __call__(self, x, verbose = False) -> Any:\n",
    "        grad = self.grad_f(self.f, x)\n",
    "        if isinstance(grad, torch.DoubleTensor): grad = grad.detach().numpy()\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        solved = grad_norm < self.suf_grad\n",
    "        #if solved and verbose:\n",
    "        if verbose: print(f\"Gradient norm {grad_norm}.\")\n",
    "        return solved \n",
    "\n",
    "\n",
    "\n",
    "class Solver():\n",
    "\n",
    "    def __init__(self, f, stop_crit, max_iter = 10_000, ro = 0.5, alpha_ini = 0.99, c = 0.45, alpha_f = None, iternad_processor = None, grad_f = jacobian, hes_f = hessian) -> None:\n",
    "        self.f = f\n",
    "        self.stop_crit = stop_crit\n",
    "        if self.stop_crit.f == None: self.stop_crit.f = f\n",
    "        self.ro = ro\n",
    "        self.alpha_ini = alpha_ini\n",
    "        self.c = c\n",
    "        self.iterands = 0\n",
    "        self.max_iter = max_iter\n",
    "        self.grad_f = grad_f\n",
    "        self.stop_crit.grad_f = grad_f\n",
    "        self.hes_f = hes_f\n",
    "        if alpha_f: self.get_alpha = alpha_f\n",
    "        self.has_iterand_processor = iternad_processor is not None\n",
    "        if iternad_processor is not None:\n",
    "            self.iterand_processor = iternad_processor\n",
    "\n",
    "    def solve(self, x):\n",
    "        x = self.tensorize(x)\n",
    "        self.iterands = 0\n",
    "        while not self.stop_crit(x) and self.iterands < self.max_iter:\n",
    "            #print(f\".x_{len(self.iterands)} = {x}\")\n",
    "            self.iterands += 1\n",
    "            if self.has_iterand_processor:\n",
    "                self.iterand_processor(x)\n",
    "            p = torch.reshape(self.get_p(x), x.shape)\n",
    "            x = x + self.get_alpha(p, x) * p\n",
    "        if self.stop_crit(x, verbose = True):\n",
    "            print(f\"Converged to the solution {x} after {self.iterands} steps\")\n",
    "        else:\n",
    "            print(f\"Failed to converge and ended in {x}\")\n",
    "        return x\n",
    "\n",
    "    def get_p(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_alpha(self, p, x):\n",
    "        # Do the line search\n",
    "        alpha = self.alpha_ini\n",
    "        while self.f(x + alpha * p) > self.f(x) + self.c * alpha * p.T @ self.grad_f(self.f,x):\n",
    "            alpha *= self.ro\n",
    "        return alpha\n",
    "\n",
    "    def tensorize(self, x):\n",
    "        if type(x) in [int, float]: return torch.DoubleTensor([x])\n",
    "        else: return torch.DoubleTensor(x)\n",
    "\n",
    "class HessianModifiedNewton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = self.hes_f(self.f, x)\n",
    "        modified_hes = self.make_positive_definite(hes)\n",
    "        down_grad = -self.grad_f(self.f, x)\n",
    "        #print(\"Approximated:\", down_grad)\n",
    "        #print(\"Exact:\", -jacobian(self.f, x))\n",
    "        p = np.linalg.solve(modified_hes, down_grad)\n",
    "        return torch.DoubleTensor(p)\n",
    "    \n",
    "    def make_positive_definite(self, H, beta = 1e-3, max_iter = 1e4):\n",
    "        # Choose tau\n",
    "        min_diag = np.min(np.diag(H))\n",
    "        tau = 0 if min_diag > 0 else beta - min_diag\n",
    "        I = np.eye(*H.shape)\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            try:\n",
    "                L = np.linalg.cholesky(H + tau * I)\n",
    "                return L @ L.T\n",
    "            except np.linalg.LinAlgError:\n",
    "                tau = max(2 * tau, beta)\n",
    "            i += 1\n",
    "\n",
    "class Newton(Solver):\n",
    "\n",
    "    def get_p(self, x):\n",
    "        hes = self.hes_f(self.f, x)\n",
    "        down_grad = -self.grad_f(self.f, x)\n",
    "        p = np.linalg.solve(hes, down_grad)\n",
    "        return torch.DoubleTensor(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 23 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 2.671230645372209.\n",
      "Failed to converge and ended in tensor([0.1948, 0.0455], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = Newton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 0.00012647082826832713.\n",
      "Failed to converge and ended in tensor([1.0000, 1.0000], dtype=torch.float64)\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.411522592282743e-06.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 22 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 2.6712299704739926.\n",
      "Failed to converge and ended in tensor([0.1948, 0.0455], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = Newton(problem_1, GradStop(problem_1, suf_grad = 1e-4), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 25.90677839441986.\n",
      "Failed to converge and ended in tensor([-0.1576,  0.7289], dtype=torch.float64)\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 34.954029214106214.\n",
      "Failed to converge and ended in tensor([1.4726, 0.0607], dtype=torch.float64)\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 0.006535527940709905.\n",
      "Failed to converge and ended in tensor([0.4366, 0.1094], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = Newton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 25.906778825647354.\n",
      "Failed to converge and ended in tensor([-0.1576,  0.7289], dtype=torch.float64)\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 34.95402396929211.\n",
      "Failed to converge and ended in tensor([1.4726, 0.0607], dtype=torch.float64)\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 0.00653564945522422.\n",
      "Failed to converge and ended in tensor([0.4366, 0.1094], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = Newton(problem_2, GradStop(problem_2), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method with Hessian Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 5.2378828278824896e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 9 steps\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.416159567795207e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 23 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 1.1491707515913118e-08.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 7 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_1, GradStop(problem_1))\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (1.2, 1.2)\n",
      "Gradient norm 0.00012647092117647635.\n",
      "Failed to converge and ended in tensor([1.0000, 1.0000], dtype=torch.float64)\n",
      "\n",
      "Starting at (-1.2, 1)\n",
      "Gradient norm 7.411539298317803e-06.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 22 steps\n",
      "\n",
      "Starting at (0.2, 0.8)\n",
      "Gradient norm 1.1467021768510661e-06.\n",
      "Converged to the solution tensor([1.0000, 1.0000], dtype=torch.float64) after 6 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_1(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_1, GradStop(problem_1, suf_grad = 1e-4), grad_f=forward_difference, hes_f = hessian_approximation_wiki)\n",
    "print(\"\\nStarting at (1.2, 1.2)\")\n",
    "x = solver.solve([1.2,1.2])\n",
    "print(\"\\nStarting at (-1.2, 1)\")\n",
    "x = solver.solve([-1.2, 1])\n",
    "print(\"\\nStarting at (0.2, 0.8)\")\n",
    "x = solver.solve([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 8.246042296740478e-08.\n",
      "Converged to the solution tensor([2.6682e-10, 1.0000e+00], dtype=torch.float64) after 7 steps\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 8.949292743208082e-07.\n",
      "Converged to the solution tensor([4.0000e+00, 1.8204e-10], dtype=torch.float64) after 6 steps\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 1.354512588685944e-08.\n",
      "Converged to the solution tensor([ 4.0000e+00, -2.9169e-12], dtype=torch.float64) after 8 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_2, GradStop(problem_2))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at (-0.2, 1.2)\n",
      "Gradient norm 8.246042296740478e-08.\n",
      "Converged to the solution tensor([2.6682e-10, 1.0000e+00], dtype=torch.float64) after 7 steps\n",
      "\n",
      "Starting at (3.8, 0.1)\n",
      "Gradient norm 8.949292743208082e-07.\n",
      "Converged to the solution tensor([4.0000e+00, 1.8204e-10], dtype=torch.float64) after 6 steps\n",
      "\n",
      "Starting at (1.9, 0.6)\n",
      "Gradient norm 1.354512588685944e-08.\n",
      "Converged to the solution tensor([ 4.0000e+00, -2.9169e-12], dtype=torch.float64) after 8 steps\n"
     ]
    }
   ],
   "source": [
    "def problem_2(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "solver = HessianModifiedNewton(problem_2, GradStop(problem_2, suf_grad = 1e-6))\n",
    "print(\"\\nStarting at (-0.2, 1.2)\")\n",
    "x = solver.solve([-0.2,1.2])\n",
    "print(\"\\nStarting at (3.8, 0.1)\")\n",
    "x = solver.solve([3.8, 0.1])\n",
    "print(\"\\nStarting at (1.9, 0.6)\")\n",
    "x = solver.solve([1.9, 0.6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
